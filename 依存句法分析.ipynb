{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 加载数据\n",
    "\n",
    "本次依存句法分析完成的是Biaffine Parser模型\n",
    "\n",
    "数据由10列组成，第0列为索引，从1开始，第1列为单词，第6列为每个词对应的head，第7列为与head组成的弧上的标签。\n",
    "下面是一句样例\n",
    "\n",
    "```txt\n",
    "1       She     _       _       _       _       2       nsubj   _       _\n",
    "2       enjoys  _       _       _       _       0       root    _       _\n",
    "3       playing _       _       _       _       2       xcomp   _       _\n",
    "4       tennis  _       _       _       _       3       dobj    _       _\n",
    "5       .       _       _       _       _       2       punct   _       _\n",
    "\n",
    "```\n",
    "\n",
    "我们直接使用supar来加载数据。\n",
    "首先，创建一下单词标签等多个域，然后用训练集初始化。\n",
    "主要是创建单词表，收集所有标签等等。\n",
    "创建完之后，可以通过域进行数字化和逆数字化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "100%|###################################8| 985021/989860 00:03<00:00, 284851.45it/s\nTotal words: 401153\nTotal words in train data: 21679\nTotal characters: 82\nTotal labels: 46\nNumericalize 'John saw Marry': [tensor([    2, 11312, 17403, 12647])]\n['<bos>', 'john', 'saw', 'marry']\n['<bos>', 'acomp', 'advcl', 'advmod', 'amod', 'appos', 'aux', 'auxpass', 'cc', 'ccomp', 'conj', 'cop', 'csubj', 'csubjpass', 'dep', 'det', 'discourse', 'dobj', 'expl', 'infmod', 'iobj', 'mark', 'mwe', 'neg', 'nn', 'npadvmod', 'nsubj', 'nsubjpass', 'num', 'number', 'parataxis', 'partmod', 'pcomp', 'pobj', 'poss', 'possessive', 'preconj', 'predet', 'prep', 'prt', 'punct', 'quantmod', 'rcmod', 'root', 'tmod', 'xcomp']\n"
    }
   ],
   "source": [
    "import torch\n",
    "from supar.utils.field import Field, SubwordField\n",
    "from supar.utils import Dataset, Embedding\n",
    "from supar.utils.metric import AttachmentMetric\n",
    "from supar.utils.transform import CoNLL\n",
    "from pprint import pprint\n",
    "\n",
    "\n",
    "WORD = Field('words', pad='<pad>', unk='<unk>', bos='<bos>', lower=True)\n",
    "CHAR = SubwordField('chars', pad='<pad>', unk='<unk>', bos='<bos>',fix_len=20)\n",
    "ARC = Field('arcs', bos='<bos>', use_vocab=False, fn=CoNLL.get_arcs)\n",
    "REL = Field('rels', bos='<bos>')\n",
    "\n",
    "transform = CoNLL(FORM=(WORD, CHAR), HEAD=ARC, DEPREL=REL)\n",
    "train = Dataset(transform, 'data/ptb/train.conllx')\n",
    "WORD.build(train, 2, Embedding.load('data/glove.6B.100d.txt', 'unk'))\n",
    "CHAR.build(train)\n",
    "REL.build(train)\n",
    "\n",
    "print(f\"\\nTotal words: {len(WORD.vocab)}\\nTotal words in train data: {WORD.vocab.n_init}\")\n",
    "print(f\"Total characters: {len(CHAR.vocab)}\")\n",
    "print(f\"Total labels: {len(REL.vocab)}\")\n",
    "\n",
    "print(f\"Numericalize 'John saw Marry': {WORD.transform([['John', 'saw', 'Marry']])}\")\n",
    "print(WORD.vocab[[2, 11312, 17403, 12647]])\n",
    "print(REL.vocab.itos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从文件中加载train/dev/test数据，简单起见，分别只保留了1000/200/200句。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Load the data\n 66%|#######################6            | 38794/59100 00:00<00:00, 387896.90it/s\ntext:\n1\tMs.\t_\tNNP\tNNP\t_\t2\tnn\t_\t_\n2\tHaag\t_\tNNP\tNNP\t_\t3\tnsubj\t_\t_\n3\tplays\t_\tVBZ\tVBZ\t_\t0\troot\t_\t_\n4\tElianti\t_\tNNP\tNNP\t_\t3\tdobj\t_\t_\n5\t.\t_\t.\t.\t_\t3\tpunct\t_\t_\n\narcs: ('2', '3', '0', '3', '3')\nrels: ('nn', 'nsubj', 'root', 'dobj', 'punct')\n"
    }
   ],
   "source": [
    "print(\"Load the data\")\n",
    "train = Dataset(transform, 'data/ptb/train.conllx')\n",
    "train.sentences = train.sentences[:1000]\n",
    "dev = Dataset(transform, 'data/ptb/dev.conllx')\n",
    "dev.sentences = dev.sentences[:200]\n",
    "test = Dataset(transform, 'data/ptb/test.conllx')\n",
    "test.sentences = test.sentences[:200]\n",
    "\n",
    "print(f\"\\ntext:\\n{train.sentences[1]}\")\n",
    "print(f\"arcs: {train.arcs[1]}\")\n",
    "print(f\"rels: {train.rels[1]}\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "根据前面建立的类为`CoNLL`的transform，对数据集进行数字化，产生一个`DataLoader`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Numericalize the data\n\ntrain: Dataset(n_sentences=1000, n_batches=32, n_buckets=32)\ndev:   Dataset(n_sentences=200, n_batches=32, n_buckets=32)\ntest:  Dataset(n_sentences=200, n_batches=32, n_buckets=32)\n\n<supar.utils.data.DataLoader object at 0x2ad9a7f60fd0>\n[tensor([[     2,   1960,  10431,   9916,   1961, 195924,  19671,  19736,  14626,\n           6471,  19884,  11146,  18377,   1959,  13335,  66371,   2010,  11950,\n             35,      8],\n        [     2,  15279,  14073,   4491,  10625,  16182,   2363,  19881,  17658,\n          11182,  16570,  17756,   2400,  19881,   4421,  19671,  21232,  13503,\n           6976,     35],\n        [     2,   2657,  12574,   9109,  12527,   9916,   4305,  20635,   4677,\n          11974,   2657,  17405,  19708,  21350,   3533,   4422,  18861,  19736,\n          21178,     35],\n        [     2,  12607,   9158,     28,   8824,  10902,     28,  10209,   4677,\n          15289,  21213,   2012,    765,      6,  10611,  17763,  14073,  10167,\n           9119,     35],\n        [     2,  19671,  21099,  13248,  10121,  19666,  19673,   1813,      6,\n           2657,   1879,      6,  11974,  10611,   3183,   2657,  17716,  14073,\n            650,     35],\n        [     2,   4403,   1961,  18991,  11971,  14073,  11093,  21417,  11146,\n          13421,  13335,  20549,  19736,  19842,   2956,     28,   9109,  12527,\n          17291,     35],\n        [     2,  10611,   2634,  20619,  13401,     28,  17763,   9119,  13402,\n          19881,   4499,  11094,  21414,  16330,  14154,  19680,  19903,  14950,\n          12084,     35]]),\n tensor([[[ 2,  0,  0,  ...,  0,  0,  0],\n         [55, 55,  0,  ...,  0,  0,  0],\n         [37,  0,  0,  ...,  0,  0,  0],\n         ...,\n         [67, 60, 74,  ...,  0,  0,  0],\n         [12,  0,  0,  ...,  0,  0,  0],\n         [ 8,  0,  0,  ...,  0,  0,  0]],\n\n        [[ 2,  0,  0,  ...,  0,  0,  0],\n         [44, 70, 73,  ...,  0,  0,  0],\n         [70, 61,  0,  ...,  0,  0,  0],\n         ...,\n         [69,  8, 75,  ...,  0,  0,  0],\n         [59, 64, 74,  ..., 59,  0,  0],\n         [12,  0,  0,  ...,  0,  0,  0]],\n\n        [[ 2,  0,  0,  ...,  0,  0,  0],\n         [29, 69, 59,  ...,  0,  0,  0],\n         [68, 56, 69,  ...,  0,  0,  0],\n         ...,\n         [75, 63, 64,  ...,  0,  0,  0],\n         [78, 60, 60,  ...,  0,  0,  0],\n         [12,  0,  0,  ...,  0,  0,  0]],\n\n        ...,\n\n        [[ 2,  0,  0,  ...,  0,  0,  0],\n         [48, 63, 56,  ...,  0,  0,  0],\n         [78, 56, 74,  ...,  0,  0,  0],\n         ...,\n         [70, 61,  0,  ...,  0,  0,  0],\n         [15, 23, 22,  ...,  0,  0,  0],\n         [12,  0,  0,  ...,  0,  0,  0]],\n\n        [[ 2,  0,  0,  ...,  0,  0,  0],\n         [30, 76, 75,  ...,  0,  0,  0],\n         [56,  0,  0,  ...,  0,  0,  0],\n         ...,\n         [68, 56, 69,  ...,  0,  0,  0],\n         [74, 56, 64,  ...,  0,  0,  0],\n         [12,  0,  0,  ...,  0,  0,  0]],\n\n        [[ 2,  0,  0,  ...,  0,  0,  0],\n         [37, 69,  0,  ...,  0,  0,  0],\n         [56, 69,  0,  ...,  0,  0,  0],\n         ...,\n         [71, 63, 70,  ...,  0,  0,  0],\n         [67, 64, 69,  ...,  0,  0,  0],\n         [12,  0,  0,  ...,  0,  0,  0]]]),\n tensor([[ 0,  3,  3,  0,  5,  3, 12,  9,  9, 12,  9, 12,  5, 12, 15, 12, 15, 16,\n          3,  3],\n        [ 0,  4,  1,  2,  5,  0,  5,  8,  6, 12, 12, 12,  8,  8, 13, 18, 18, 18,\n         14,  5],\n        [ 0,  6,  4,  4,  6,  6,  0,  6,  9,  6,  6,  6, 15, 15, 15, 11, 15, 18,\n         15,  6],\n        [ 0,  2,  7,  7,  7,  4,  7,  0,  9,  7, 11,  9, 13, 11,  7, 14, 15, 18,\n         16,  7],\n        [ 0,  4,  4,  4,  0,  4, 12,  8, 12,  8,  8, 10,  5, 12, 13, 14, 14, 14,\n         17,  4],\n        [ 0, 18,  4,  4, 11,  4,  7,  5, 11, 10, 11, 18, 13, 11, 13, 18, 17, 18,\n          0, 18],\n        [ 0,  8,  4,  4,  1,  8,  7,  8,  0, 10,  8, 10, 10, 12, 10, 18, 18, 18,\n         14,  8]]),\n tensor([[ 0, 40, 26, 43, 15, 17, 21, 15,  4, 26, 44, 11,  9, 40,  4, 14, 38, 33,\n         40, 40],\n        [ 0, 14, 38, 33, 26, 43, 17,  6, 19, 34,  4, 28, 17, 38, 33, 27,  7, 23,\n         42, 40],\n        [ 0,  8,  4, 24, 26,  6, 43, 39, 24, 17,  8, 10, 26,  6,  6,  9, 17, 15,\n         44, 40],\n        [ 0, 24, 26, 40, 38, 33, 40, 43, 24, 17,  3, 38, 28, 33, 38, 33, 38, 34,\n         33, 40],\n        [ 0, 26, 11,  3, 43, 38, 15, 28, 28,  8, 10,  4, 33, 38, 33,  8, 10, 38,\n         33, 40],\n        [ 0,  8, 15,  4, 26, 38, 24, 33, 11,  3,  3, 14, 15, 44,  3, 40, 24, 26,\n         43, 40],\n        [ 0, 38, 15,  4, 33, 40,  4, 26, 43,  6, 45, 17, 38, 33, 38, 34,  4, 24,\n         33, 40]])]\n"
    }
   ],
   "source": [
    "print(\"Numericalize the data\")\n",
    "train.build(batch_size=5000, n_buckets=32, shuffle=True)\n",
    "dev.build(batch_size=5000, n_buckets=32)\n",
    "test.build(batch_size=5000, n_buckets=32)\n",
    "print(f\"\\n{'train:':6} {train}\\n{'dev:':6} {dev}\\n{'test:':6} {test}\\n\")\n",
    "print(test.loader)  \n",
    "pprint(next(iter(test.loader)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型定义\n",
    "\n",
    "定义`BiaffineParserModel`，为PyTorch的一个模块类，实现了`forward`过程，以及loss的计算，解码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from supar.modules import MLP, BertEmbedding, Biaffine, BiLSTM, CharLSTM\n",
    "from supar.modules.dropout import IndependentDropout, SharedDropout\n",
    "from supar.utils import Config\n",
    "from supar.utils.alg import eisner, eisner2o, mst\n",
    "from supar.utils.transform import CoNLL\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "\n",
    "class BiaffineDependencyModel(nn.Module):\n",
    "    \"\"\"\n",
    "    The implementation of Biaffine Dependency Parser.\n",
    "\n",
    "    References:\n",
    "        - Timothy Dozat and Christopher D. Manning (ICLR'17)\n",
    "          Deep Biaffine Attention for Neural Dependency Parsing\n",
    "          https://openreview.net/pdf?id=Hk95PK9le/\n",
    "\n",
    "    Args:\n",
    "        n_words (int):\n",
    "            Size of the word vocabulary.\n",
    "        n_feats (int):\n",
    "            Size of the feat vocabulary.\n",
    "        n_rels (int):\n",
    "            Number of labels in the treebank.\n",
    "        feat (str):\n",
    "            Specifies which type of additional feature to use: 'char' | 'bert' | 'tag'.\n",
    "            'char': Character-level representations extracted by CharLSTM.\n",
    "            'bert': BERT representations, other pretrained langugae models like `XLNet` are also feasible.\n",
    "            'tag': POS tag embeddings.\n",
    "            Default: 'char'.\n",
    "        n_embed (int):\n",
    "            Size of word embeddings. Default: 100.\n",
    "        n_feat_embed (int):\n",
    "            Size of feature representations. Default: 100.\n",
    "        n_char_embed (int):\n",
    "            Size of character embeddings serving as inputs of CharLSTM, required if feat='char'. Default: 50.\n",
    "        bert (str):\n",
    "            Specify which kind of language model to use, e.g., 'bert-base-cased' and 'xlnet-base-cased'.\n",
    "            This is required if feat='bert'. The full list can be found in `transformers`.\n",
    "            Default: `None`.\n",
    "        n_bert_layers (int):\n",
    "            Specify how many last layers to use. Required if feat='bert'.\n",
    "            The final outputs would be the weight sum of the hidden states of these layers.\n",
    "            Default: 4.\n",
    "        mix_dropout (float):\n",
    "            Dropout ratio of BERT layers. Required if feat='bert'. Default: .0.\n",
    "        embed_dropout (float):\n",
    "            Dropout ratio of input embeddings. Default: .33.\n",
    "        n_lstm_hidden (int):\n",
    "            Dimension of LSTM hidden states. Default: 400.\n",
    "        n_lstm_layers (int):\n",
    "            Number of LSTM layers. Default: 3.\n",
    "        lstm_dropout (float): Default: .33.\n",
    "            Dropout ratio of LSTM.\n",
    "        n_mlp_arc (int):\n",
    "            Arc MLP size. Default: 500.\n",
    "        n_mlp_rel  (int):\n",
    "            Label MLP size. Default: 100.\n",
    "        mlp_dropout (float):\n",
    "            Dropout ratio of MLP layers. Default: .33.\n",
    "        feat_pad_index (int):\n",
    "            The index of the padding token in the feat vocabulary. Default: 0.\n",
    "        pad_index (int):\n",
    "            The index of the padding token in the word vocabulary. Default: 0.\n",
    "        unk_index (int):\n",
    "            The index of the unknown token in the word vocabulary. Default: 1.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 n_words,\n",
    "                 n_feats,\n",
    "                 n_rels,\n",
    "                 feat='char',\n",
    "                 n_embed=100,\n",
    "                 n_feat_embed=100,\n",
    "                 n_char_embed=50,\n",
    "                 bert=None,\n",
    "                 n_bert_layers=4,\n",
    "                 mix_dropout=.0,\n",
    "                 embed_dropout=.33,\n",
    "                 n_lstm_hidden=400,\n",
    "                 n_lstm_layers=3,\n",
    "                 lstm_dropout=.33,\n",
    "                 n_mlp_arc=500,\n",
    "                 n_mlp_rel=100,\n",
    "                 mlp_dropout=.33,\n",
    "                 feat_pad_index=0,\n",
    "                 pad_index=0,\n",
    "                 unk_index=1,\n",
    "                 **kwargs):\n",
    "        super().__init__()\n",
    "\n",
    "        self.args = Config().update(locals())\n",
    "        # the embedding layer\n",
    "        self.word_embed = nn.Embedding(num_embeddings=n_words,\n",
    "                                       embedding_dim=n_embed)\n",
    "\n",
    "        self.feat_embed = CharLSTM(n_chars=n_feats,\n",
    "                                       n_embed=n_char_embed,\n",
    "                                       n_out=n_feat_embed,\n",
    "                                       pad_index=feat_pad_index)\n",
    "        self.embed_dropout = IndependentDropout(p=embed_dropout)\n",
    "\n",
    "        # the lstm layer\n",
    "        self.lstm = BiLSTM(input_size=n_embed+n_feat_embed,\n",
    "                           hidden_size=n_lstm_hidden,\n",
    "                           num_layers=n_lstm_layers,\n",
    "                           dropout=lstm_dropout)\n",
    "        self.lstm_dropout = SharedDropout(p=lstm_dropout)\n",
    "\n",
    "        # the MLP layers\n",
    "        self.mlp_arc_d = MLP(n_in=n_lstm_hidden*2,\n",
    "                             n_out=n_mlp_arc,\n",
    "                             dropout=mlp_dropout)\n",
    "        self.mlp_arc_h = MLP(n_in=n_lstm_hidden*2,\n",
    "                             n_out=n_mlp_arc,\n",
    "                             dropout=mlp_dropout)\n",
    "        self.mlp_rel_d = MLP(n_in=n_lstm_hidden*2,\n",
    "                             n_out=n_mlp_rel,\n",
    "                             dropout=mlp_dropout)\n",
    "        self.mlp_rel_h = MLP(n_in=n_lstm_hidden*2,\n",
    "                             n_out=n_mlp_rel,\n",
    "                             dropout=mlp_dropout)\n",
    "\n",
    "        # the Biaffine layers\n",
    "        self.arc_attn = Biaffine(n_in=n_mlp_arc,\n",
    "                                 bias_x=True,\n",
    "                                 bias_y=False)\n",
    "        self.rel_attn = Biaffine(n_in=n_mlp_rel,\n",
    "                                 n_out=n_rels,\n",
    "                                 bias_x=True,\n",
    "                                 bias_y=True)\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.pad_index = pad_index\n",
    "        self.unk_index = unk_index\n",
    "\n",
    "    def load_pretrained(self, embed=None):\n",
    "        if embed is not None:\n",
    "            self.pretrained = nn.Embedding.from_pretrained(embed)\n",
    "            nn.init.zeros_(self.word_embed.weight)\n",
    "        return self\n",
    "\n",
    "    def forward(self, words, feats):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            words (LongTensor) [batch_size, seq_len]:\n",
    "                The word indices.\n",
    "            feats (LongTensor):\n",
    "                The feat indices.\n",
    "                If feat is 'char' or 'bert', the size of feats should be [batch_size, seq_len, fix_len]\n",
    "                If 'tag', then the size is [batch_size, seq_len].\n",
    "\n",
    "        Returns:\n",
    "            s_arc (Tensor): [batch_size, seq_len, seq_len]\n",
    "                The scores of all possible arcs.\n",
    "            s_rel (Tensor): [batch_size, seq_len, seq_len, n_labels]\n",
    "                The scores of all possible labels on each arc.\n",
    "        \"\"\"\n",
    "\n",
    "        batch_size, seq_len = words.shape\n",
    "        # get the mask and lengths of given batch\n",
    "        mask = words.ne(self.pad_index)\n",
    "        ext_words = words\n",
    "        # set the indices larger than num_embeddings to unk_index\n",
    "        if hasattr(self, 'pretrained'):\n",
    "            ext_mask = words.ge(self.word_embed.num_embeddings)\n",
    "            ext_words = words.masked_fill(ext_mask, self.unk_index)\n",
    "\n",
    "        # get outputs from embedding layers\n",
    "        word_embed = self.word_embed(ext_words)\n",
    "        if hasattr(self, 'pretrained'):\n",
    "            word_embed += self.pretrained(words)\n",
    "        feat_embed = self.feat_embed(feats)\n",
    "        word_embed, feat_embed = self.embed_dropout(word_embed, feat_embed)\n",
    "        # concatenate the word and feat representations\n",
    "        embed = torch.cat((word_embed, feat_embed), -1)\n",
    "\n",
    "        x = pack_padded_sequence(embed, mask.sum(1), True, False)\n",
    "        x, _ = self.lstm(x)\n",
    "        x, _ = pad_packed_sequence(x, True, total_length=seq_len)\n",
    "        x = self.lstm_dropout(x)\n",
    "\n",
    "        # apply MLPs to the BiLSTM output states\n",
    "        arc_d = self.mlp_arc_d(x)\n",
    "        arc_h = self.mlp_arc_h(x)\n",
    "        rel_d = self.mlp_rel_d(x)\n",
    "        rel_h = self.mlp_rel_h(x)\n",
    "\n",
    "        # [batch_size, seq_len, seq_len]\n",
    "        s_arc = self.arc_attn(arc_d, arc_h)\n",
    "        # [batch_size, seq_len, seq_len, n_rels]\n",
    "        s_rel = self.rel_attn(rel_d, rel_h).permute(0, 2, 3, 1)\n",
    "        # set the scores that exceed the length of each sentence to -inf\n",
    "        s_arc.masked_fill_(~mask.unsqueeze(1), float('-inf'))\n",
    "\n",
    "        return s_arc, s_rel\n",
    "\n",
    "    def loss(self, s_arc, s_rel, arcs, rels, mask):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            s_arc (Tensor): [batch_size, seq_len, seq_len]\n",
    "                The scores of all possible arcs.\n",
    "            s_rel (Tensor): [batch_size, seq_len, seq_len, n_labels]\n",
    "                The scores of all possible labels on each arc.\n",
    "            arcs (LongTensor): [batch_size, seq_len]\n",
    "                Tensor of gold-standard arcs.\n",
    "            rels (LongTensor): [batch_size, seq_len]\n",
    "                Tensor of gold-standard labels.\n",
    "            mask (BoolTensor): [batch_size, seq_len, seq_len]\n",
    "                Mask for covering the unpadded tokens.\n",
    "\n",
    "        Returns:\n",
    "            loss (Tensor): scalar\n",
    "                The training loss.\n",
    "        \"\"\"\n",
    "        s_arc, arcs = s_arc[mask], arcs[mask]    \n",
    "        s_rel, rels = s_rel[mask], rels[mask]\n",
    "        # s_rel: [batch_size, seq_len, seq_len, n_labels] \n",
    "        # s_rel[mask] -> [L, seq_len, n_labels]\n",
    "        # s_rel[torch.arange(len(arcs)), arcs] -> [L, n_labels]\n",
    "\n",
    "        s_rel = s_rel[torch.arange(len(arcs)), arcs]\n",
    "        arc_loss = self.criterion(s_arc, arcs)\n",
    "        rel_loss = self.criterion(s_rel, rels)\n",
    "\n",
    "        return arc_loss + rel_loss\n",
    "\n",
    "    def decode(self, s_arc, s_rel, mask, tree=False, proj=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            s_arc (Tensor): [batch_size, seq_len, seq_len]\n",
    "                The scores of all possible arcs.\n",
    "            s_rel (Tensor): [batch_size, seq_len, seq_len, n_labels]\n",
    "                The scores of all possible labels on each arc.\n",
    "            mask (BoolTensor): [batch_size, seq_len, seq_len]\n",
    "                Mask for covering the unpadded tokens.\n",
    "            tree (bool):\n",
    "                If True, ensures to output well-formed trees. Default: False.\n",
    "            proj (bool):\n",
    "                If True, ensures to output projective trees. Default: False.\n",
    "\n",
    "        Returns:\n",
    "            arc_preds (Tensor): [batch_size, seq_len]\n",
    "                The predicted arcs.\n",
    "            rel_preds (Tensor): [batch_size, seq_len]\n",
    "                The predicted labels.\n",
    "        \"\"\"\n",
    "\n",
    "        lens = mask.sum(1)  \n",
    "        # prevent self-loops\n",
    "        s_arc.diagonal(0, 1, 2).fill_(float('-inf'))\n",
    "        arc_preds = mst(s_arc, mask)\n",
    "        rel_preds = s_rel.argmax(-1).gather(-1, arc_preds.unsqueeze(-1)).squeeze(-1)\n",
    "\n",
    "        return arc_preds, rel_preds\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, path):\n",
    "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        state = torch.load(path, map_location=device)\n",
    "        model = cls(**state['args'])\n",
    "        model.load_pretrained(state['pretrained'])\n",
    "        model.load_state_dict(state['state_dict'], False)\n",
    "        model.to(device)\n",
    "\n",
    "        return model\n",
    "\n",
    "    def save(self, path):\n",
    "        state_dict, pretrained = self.state_dict(), None\n",
    "        if hasattr(self, 'pretrained'):\n",
    "            pretrained = state_dict.pop('pretrained.weight')\n",
    "        state = {\n",
    "            'args': self.args,\n",
    "            'state_dict': state_dict,\n",
    "            'pretrained': pretrained\n",
    "        }\n",
    "        torch.save(state, path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "创建optimizer，和控制学习速率衰减的scheduler，采用指数衰减，衰减公式为$0.75^\\frac{t}{5000}$\n",
    "创建前面定义的模型，参数均设为默认"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "BiaffineDependencyModel(\n  (word_embed): Embedding(21679, 100)\n  (feat_embed): CharLSTM(82, 50, n_out=100, pad_index=0)\n  (embed_dropout): IndependentDropout(p=0.33)\n  (lstm): BiLSTM(200, 400, num_layers=3, dropout=0.33)\n  (lstm_dropout): SharedDropout(p=0.33, batch_first=True)\n  (mlp_arc_d): MLP(n_in=800, n_out=500, dropout=0.33)\n  (mlp_arc_h): MLP(n_in=800, n_out=500, dropout=0.33)\n  (mlp_rel_d): MLP(n_in=800, n_out=100, dropout=0.33)\n  (mlp_rel_h): MLP(n_in=800, n_out=100, dropout=0.33)\n  (arc_attn): Biaffine(n_in=500, n_out=1, bias_x=True)\n  (rel_attn): Biaffine(n_in=100, n_out=46, bias_x=True, bias_y=True)\n  (criterion): CrossEntropyLoss()\n  (pretrained): Embedding(401153, 100)\n)\n"
    }
   ],
   "source": [
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "\n",
    "model = BiaffineDependencyModel(n_words=WORD.vocab.n_init,\n",
    "                                n_feats=len(CHAR.vocab), \n",
    "                                n_rels=len(REL.vocab), \n",
    "                                pad_index=WORD.pad_index, \n",
    "                                unk_index=WORD.unk_index, \n",
    "                                bos_index=WORD.bos_index, \n",
    "                                feat_pad_index=CHAR.pad_index)\n",
    "model.load_pretrained(WORD.embed)\n",
    "optimizer = Adam(model.parameters(), lr=2e-3, betas=(0.9, 0.9), eps=1e-12)\n",
    "scheduler = ExponentialLR(optimizer, .75**(1/5000))\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练模型\n",
    "\n",
    "首先定义评价指标。\n",
    "对ptb数据集而言，评价时需要去掉标点。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "UCM:  0.00% LCM:  0.00% UAS: 80.00% LAS: 60.00%\ntensor([     3,      4,      6,      7,      8,      9,     26,     27,     28,\n            29,     30,     35,     36,   1955,   1956,   1958,  21679,  21680,\n         21681,  21682,  21683,  21684,  21685,  21686,  21687,  21813,  21979,\n         21980,  22266,  22267,  22268,  22269,  22270,  22271,  22272,  22273,\n         22274,  22275,  22276,  22277,  22278,  22279,  22280,  22281,  22282,\n         22283,  22284,  22285,  22286,  22287,  22288,  22289,  22290,  22291,\n         22292,  22293,  22294,  22295,  22296,  22297,  22298,  22299,  22300,\n         22301,  22302,  22303,  22304,  22305,  22306,  22307,  22517,  22518,\n         23328,  62761,  62762,  62763,  62838,  62839,  62843,  62844,  62854,\n         62855,  62856,  62857,  62858,  62859,  62860,  62862,  62863,  62864,\n         62866,  62867,  62868,  62869,  62870,  62871,  62872,  62873,  62874,\n         62875,  62876,  62877,  62878,  62879,  62880,  62881,  62882,  62883,\n         62884,  62885,  62886,  62887,  62888,  62889,  62890,  62891,  62892,\n         62893,  62894,  62895,  62896,  62897, 399866, 399868, 399870, 399877,\n        399878, 399879, 399883, 401059, 401060, 401061, 401062, 401063, 401064,\n        401065, 401079, 401080, 401081, 401082, 401083, 401084, 401085, 401086,\n        401087, 401088, 401089, 401090, 401091, 401092, 401093, 401094, 401095,\n        401106, 401107])\n['!', '#', '%', '&', \"'\", \"''\", '*', '**', ',', '-', '--', '.', '...', ':', ';', '?', '!!', '!!!', '!!!!', '!!!!!', '!?', '!?!', '\"', '##', '###', '(', ')', '***', '---', '----', '-----', '------', '-------', '--------', '---------', '----------', '-----------', '------------', '-------------', '--------------', '---------------', '----------------', '-----------------', '------------------', '-------------------', '--------------------', '---------------------', '----------------------', '-----------------------', '------------------------', '-------------------------', '--------------------------', '----------------------------', '-----------------------------', '--------------------------------', '---------------------------------', '---------------------------------------', '----------------------------------------------', '-----------------------------------------------', '------------------------------------------------', '-------------------------------------------------', '--------------------------------------------------', '---------------------------------------------------', '----------------------------------------------------', '------------------------------------------------------', '-------------------------------------------------------', '---------------------------------------------------------', '-----------------------------------------------------------------', '------------------------------------------------------------------', '--------------------------------------------------------------------', '....', '.....', '/', ':(', ':)', ':-)', ':@', ':]', ';(', ';)', '?!', '?!?', '??', '???', '????', '?????', '@', '[', '\\\\', ']', '_', '__', '___', '____', '_____', '______', '_______', '________', '_________', '__________', '___________', '____________', '_____________', '_______________', '________________', '___________________', '________________________', '_____________________________', '______________________________', '_______________________________', '________________________________', '_________________________________', '__________________________________', '___________________________________', '____________________________________', '_____________________________________', '______________________________________', '_______________________________________', '_________________________________________', '____________________________________________', '_____________________________________________', '___________________________________________________________', '{', '}', '¡', '«', '·', '»', '¿', '–', '—', '―', '‘', '‘‘', '‘’', '’', '’‘', '’’', '’”', '‚', '“', '“‘', '“’', '““', '“”', '”', '”’', '””', '„', '‟', '…', '‹', '›', '、', '。']\ntensor([[ True,  True, False, False,  True]])\nUCM:  0.00% LCM:  0.00% UAS: 66.67% LAS: 66.67%\n"
    }
   ],
   "source": [
    "from supar.utils.metric import Metric\n",
    "from supar.utils.fn import ispunct\n",
    "\n",
    "\n",
    "class AttachmentMetric(Metric):\n",
    "\n",
    "    def __init__(self, eps=1e-8):\n",
    "        super().__init__()\n",
    "\n",
    "        self.eps = eps\n",
    "\n",
    "        self.n = 0.0\n",
    "        self.n_ucm = 0.0\n",
    "        self.n_lcm = 0.0\n",
    "        self.total = 0.0\n",
    "        self.correct_arcs = 0.0\n",
    "        self.correct_rels = 0.0\n",
    "\n",
    "    def __repr__(self):\n",
    "        s = f\"UCM: {self.ucm:6.2%} LCM: {self.lcm:6.2%} \"\n",
    "        s += f\"UAS: {self.uas:6.2%} LAS: {self.las:6.2%}\"\n",
    "        return s\n",
    "\n",
    "    def __call__(self, arc_preds, rel_preds, arc_golds, rel_golds, mask):\n",
    "        lens = mask.sum(1)\n",
    "        arc_mask = arc_preds.eq(arc_golds) & mask\n",
    "        rel_mask = rel_preds.eq(rel_golds) & arc_mask\n",
    "        arc_mask_seq, rel_mask_seq = arc_mask[mask], rel_mask[mask]\n",
    "\n",
    "        self.n += len(mask)\n",
    "        self.n_ucm += arc_mask.sum(1).eq(lens).sum().item()\n",
    "        self.n_lcm += rel_mask.sum(1).eq(lens).sum().item()\n",
    "\n",
    "        self.total += len(arc_mask_seq)\n",
    "        self.correct_arcs += arc_mask_seq.sum().item()\n",
    "        self.correct_rels += rel_mask_seq.sum().item()\n",
    "\n",
    "    @property\n",
    "    def score(self):\n",
    "        return self.las\n",
    "\n",
    "    @property\n",
    "    def ucm(self):\n",
    "        return self.n_ucm / (self.n + self.eps)\n",
    "\n",
    "    @property\n",
    "    def lcm(self):\n",
    "        return self.n_lcm / (self.n + self.eps)\n",
    "\n",
    "    @property\n",
    "    def uas(self):\n",
    "        return self.correct_arcs / (self.total + self.eps)\n",
    "\n",
    "    @property\n",
    "    def las(self):\n",
    "        return self.correct_rels / (self.total + self.eps)\n",
    "\n",
    "\n",
    "\n",
    "metric = AttachmentMetric()\n",
    "metric(torch.tensor([[1, 2, 3, 4, 4]]), # arc_preds\n",
    "       torch.tensor([[1, 2, 3, 3, 5]]), # rel_preds\n",
    "       torch.tensor([[1, 2, 3, 4, 5]]), # arc_golds\n",
    "       torch.tensor([[1, 2, 3, 4, 5]]), # rel_golds\n",
    "       torch.ones((1, 5)).gt(0))\n",
    "print(metric)\n",
    "puncts = torch.tensor([i for s, i in WORD.vocab.stoi.items() if ispunct(s)])\n",
    "mask = torch.ones((1, 5)).gt(0) & torch.tensor([[1, 2, 3, 4, 5]]).unsqueeze(-1).ne(puncts).all(-1)\n",
    "print(puncts)\n",
    "print(WORD.vocab[puncts])\n",
    "print(mask)\n",
    "metric = AttachmentMetric()\n",
    "metric(torch.tensor([[1, 2, 3, 4, 4]]), \n",
    "       torch.tensor([[1, 2, 3, 3, 5]]), \n",
    "       torch.tensor([[1, 2, 3, 4, 5]]),\n",
    "       torch.tensor([[1, 2, 3, 4, 5]]),\n",
    "       mask)\n",
    "print(metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面定义了`train_loader`和`evaluate_loader`函数，遍历loader分别进行训练和评价，作为一个epoch。\n",
    "另外还有`train_parser`函数，训练，评价和保存模型。\n",
    "简单起见设置`patience`为3，要得到更好的性能，可以设为100。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Epoch 1 saved\ndev:   - <supar.utils.metric.Metric object at 0x2ad986ba3910>\ntest:  - UCM: 19.50% LCM:  9.50% UAS: 81.08% LAS: 75.67%\n"
    },
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'epoch' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-95feaefd8079>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{elapsed}s elapsed, {elapsed / epoch}s/epoch\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m \u001b[0mtrain_parser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdev\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-33-95feaefd8079>\u001b[0m in \u001b[0;36mtrain_parser\u001b[0;34m(train, dev, test, model, optimizer, scheduler, path, epochs, patience)\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{'dev:':6} - {best_metric}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{'test:':6} - {metric}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{elapsed}s elapsed, {elapsed / epoch}s/epoch\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0mtrain_parser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdev\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'epoch' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "\n",
    "def train_loader(model, optimizer, scheduler, loader):\n",
    "    model.train()\n",
    "\n",
    "    metric = AttachmentMetric()\n",
    "\n",
    "    for words, feats, arcs, rels in loader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        mask = words.ne(WORD.pad_index)\n",
    "        # ignore the first token of each sentence\n",
    "        mask[:, 0] = 0\n",
    "        s_arc, s_rel = model(words, feats)\n",
    "        loss = model.loss(s_arc, s_rel, arcs, rels, mask)\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), 5.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        arc_preds, rel_preds = model.decode(s_arc, s_rel, mask)\n",
    "        # ignore all punctuation if not specified\n",
    "        # mask &= words.unsqueeze(-1).ne(self.puncts).all(-1)\n",
    "        metric(arc_preds, rel_preds, arcs, rels, mask)\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_loader(model, loader):\n",
    "    model.eval()\n",
    "\n",
    "    total_loss, metric = 0, AttachmentMetric()\n",
    "\n",
    "    for words, feats, arcs, rels in loader:\n",
    "        mask = words.ne(WORD.pad_index)\n",
    "        # ignore the first token of each sentence\n",
    "        mask[:, 0] = 0\n",
    "        s_arc, s_rel = model(words, feats)\n",
    "        loss = model.loss(s_arc, s_rel, arcs, rels, mask)\n",
    "        arc_preds, rel_preds = model.decode(s_arc, s_rel, mask)\n",
    "        total_loss += loss.item()\n",
    "        mask &= words.unsqueeze(-1).ne(puncts).all(-1)\n",
    "        metric(arc_preds, rel_preds, arcs, rels, mask)\n",
    "    total_loss /= len(loader)\n",
    "\n",
    "    return total_loss, metric\n",
    "\n",
    "def train_parser(train, dev, test, model, optimizer, scheduler,\n",
    "          path='model',\n",
    "          epochs=5000,\n",
    "          patience=3):\n",
    "    transform.train()\n",
    "\n",
    "    elapsed = timedelta()\n",
    "    best_e, best_metric = 1, Metric()\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        start = datetime.now()\n",
    "\n",
    "        print(f\"Epoch {epoch} / {epochs}:\")\n",
    "        train_loader(model, optimizer, scheduler, train.loader)\n",
    "        loss, dev_metric = evaluate_loader(model, dev.loader)\n",
    "        print(f\"{'dev:':6} - loss: {loss:.4f} - {dev_metric}\")\n",
    "        loss, test_metric = evaluate_loader(model, test.loader)\n",
    "        print(f\"{'test:':6} - loss: {loss:.4f} - {test_metric}\")\n",
    "\n",
    "        t = datetime.now() - start\n",
    "        # save the model if it is the best so far\n",
    "        if dev_metric > best_metric:\n",
    "            best_e, best_metric = epoch, dev_metric\n",
    "            model.save(path)\n",
    "            print(f\"{t}s elapsed (saved)\\n\")\n",
    "        else:\n",
    "            print(f\"{t}s elapsed\\n\")\n",
    "        elapsed += t\n",
    "        if epoch - best_e >= patience:\n",
    "            break\n",
    "    loss, metric = evaluate_loader(model.load(path), test.loader)\n",
    "\n",
    "    print(f\"Epoch {best_e} saved\")\n",
    "    print(f\"{'dev:':6} - {best_metric}\")\n",
    "    print(f\"{'test:':6} - {metric}\")\n",
    "    print(f\"{elapsed}s elapsed, {elapsed / epoch}s/epoch\")\n",
    "\n",
    "train_parser(train,dev,test,model, optimizer, scheduler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}